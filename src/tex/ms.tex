% Define document class
\documentclass[reprint,superscriptaddress,nobibnotes,amsmath,amssymb,aps,prx,hidelinks]{revtex4-2}

% Import relevant packages
\usepackage{showyourwork}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{multirow}
\usepackage[separate-uncertainty=true]{siunitx}
\usepackage{amsmath}
\usepackage[shortlabels]{enumitem}
\usepackage[page]{appendix}
\usepackage{xfrac}
\usepackage{printlen}
\usepackage[version=4]{mhchem}
\usepackage{blindtext}
% \usepackage[hidelinks]{hyperref}
\sisetup{
    list-units      = brackets,
    range-units     = brackets,
    range-phrase    = -,
    list-pair-separator= {, },
    list-separator  = {, },
    list-final-separator = {, }
    }
\renewcommand\appendixname{}
\newcommand{\papertitle}{Accurate Estimation of Diffusion Coefficients and their Uncertainties from Computer Simulation}
% \newcommand{\oMSD}{\ensuremath{\overline{\bm{s}^2}}}
% \newcommand{\oMSDs}[1]{\ensuremath{\overline{s^2}}(#1)}
% \newcommand{\oMSDi}{\ensuremath{\overline{s^2}_i}}
% \newcommand{\oMSDj}{\ensuremath{\overline{s^2}_j}}

\newcommand{\oMSD}{\ensuremath{\bm{x}}}
\newcommand{\oMSDs}[1]{\ensuremath{x}(#1)}
\newcommand{\oMSDi}{\ensuremath{x_i}}
\newcommand{\oMSDj}{\ensuremath{x_j}}
\newcommand{\oMSDn}{\ensuremath{x_n}}
\newcommand{\oMSDm}{\ensuremath{x_{n + m}}}
\newcommand{\moMSDn}{\ensuremath{\left<\oMSDn\right>}}
\newcommand{\moMSDm}{\ensuremath{\left<\oMSDm\right>}}
\newcommand{\modelmatrix}{\mathbf{A}}
\newcommand{\model}{\bm{m}}
\newcommand{\prob}[1]{\ensuremath{p(#1)}}
\newcommand{\probsq}[1]{\ensuremath{p[#1]}}
\newcommand{\nind}[1]{\ensuremath{N^\prime_{#1}}}
\newcommand{\code}[1]{#1}

\newcommand{\MSD}[1]{\big\langle\Delta\mathbf{r}{(#1)}^2\big\rangle}
\newcommand{\Dest}{\ensuremath{\widehat{D}^*}}
\newcommand{\D}{\ensuremath{D^*}}
\newcommand{\var}[1]{\ensuremath{\sigma^2[#1]}}
\newcommand{\varest}[1]{\ensuremath{\widehat{\sigma}^2[#1]}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator*{\argmax}{argmax}

% Redefine \maketitle so that it can be used twice (for supplementary)
\makeatletter
\def\maketitle{
\@author@finish
\title@column\titleblock@produce
\suppressfloats[t]}
\makeatother

% Begin!
\begin{document}

\let\oldaddcontentsline\addcontentsline% Store \addcontentsline
\renewcommand{\addcontentsline}[3]{}

% Title
\title{\papertitle}

% Author list
\author{Andrew R. McCluskey}
  \email{andrew.mccluskey@ess.eu}
  \affiliation{European Spallation Source ERIC, Ole Maaløes vej 3, 2200 København N, DK}
\author{Samuel W. Coles}
  \affiliation{Department of Chemistry, University of Bath, Claverton Down, Bath, BA2 7AY, UK}
  \affiliation{The Faraday Institution, Quad One, Harwell Science and Innovation Campus, Didcot, OX11 0RA, UK}
\author{Benjamin J. Morgan}
  \email{b.j.morgan@bath.ac.uk}
  \affiliation{Department of Chemistry, University of Bath, Claverton Down, Bath, BA2 7AY, UK}
  \affiliation{The Faraday Institution, Quad One, Harwell Science and Innovation Campus, Didcot, OX11 0RA, UK}

% Abstract
\begin{abstract}
    Self-diffusion coefficients, $\D$, are routinely estimated from molecular dynamics simulations by fitting a linear model to the observed mean-squared displacements (MSDs) of mobile species.
    MSDs derived from simulation suffer from statistical noise, which introduces uncertainty in the resulting estimate of $\D$.
    An optimal scheme for estimating $\D$ will minimise this uncertainty, i.e., will have high statistical efficiency, and will give an accurate estimate of the uncertainty itself.
    We present a scheme for estimating $\D$ from a single simulation trajectory with high statistical efficiency and accurately estimating the uncertainty in the predicted value.
    The statistical distribution of MSDs observable from a given simulation is modelled as a multivariate normal distribution using an analytical covariance matrix for an equivalent system of freely diffusing particles, which we parameterise from the available simulation data. 
    We then perform Bayesian regression to sample the distribution of linear models that are compatible with this model multivariate normal distribution, to obtain a statistically efficient estimate of $D^*$ and an accurate estimate of the associated statistical uncertainty.
\end{abstract}

\maketitle 

\section{Introduction}

Mass transport is a fundamental physical process that is central to our understanding of fluids~\cite{sendner_interfacial_2009,shimizu_structural_2015,ghoufi_ultrafast_2016} and plays a critical role in biochemical systems~\cite{maccmmon_dynamics_1977,robertson_diffusion_2006}, and in solid-state devices such as batteries, fuel cells, and chemical sensors~\cite{eames_ionic_2015,morgan_understanding_2011,walsh_taking_2018}.
Molecular dynamics simulations are widely used to study microscopic transport processes, as they give direct insight into atomic-scale transport mechanisms and can be used to calculate macroscopic transport coefficients~\cite{morgan_relationships_2014,morgan_mechanistic_2021,poletayev_defect_2022,klepis_long_2009,wang_application_2011,zelovich_hydroxide_2019}.
These transport coefficients are formally defined in terms of ensemble averages.
Dynamical simulations, however, sample the full ensemble space stochastically, and parameters derived from simulation data are therefore only estimates of the true parameter of interest.
The statistical uncertainty associated with such estimates depends on the details of the simulation---e.g., size and timescale---and on the choice of estimation method.
An optimal estimation method will minimise the uncertainty in the computed quantity, i.e., it will have high statistical efficiency, and will also allow this uncertainty to be accurately estimated.

One commonly used parameter for quantifying atomic-scale mass transport is the self-diffusion coefficient, $\D$, which describes diffusion in the absence of a chemical potential gradient.
$\D$ is related to the ensemble-average mean squared displacement (MSD), $\MSD{t}$, via the Einstein relation~\cite{einstein_uber_1905,helfand_transport_1960},
%
\begin{equation}
    \D = \lim_{t\to\infty}\frac{\MSD{t}}{6t},
    \label{equ:einstein}
\end{equation}
%
where $t$ is elapsed time.
Because numerical simulations are finite in both time and space, MSDs obtained from simulation data always deviate from the true ensemble average MSD.
One can, however, compute an estimate of the self-diffusion coefficient, $\Dest$, by fitting a linear model to the observed MSD and using the gradient of this fitted model in place of $\MSD{t} / t$ in Eqn.~\ref{equ:einstein}.

The simplest approach to fitting a linear model to MSD data from simulation is ordinary least squares regression (OLS).
OLS gives analytical expressions for the ``best fit'' regression coefficients (the slope and intercept) and their respective uncertainties, making it easy to implement and quick to perform.
This procedure, however, is appropriate only for data that are both statistically independent and identically distributed.
Neither of these conditions hold for MSD data obtained from simulation, which instead are serially correlated and usually have unequal variances.
As a consequence, OLS is statistically inefficient, giving a relatively large statistical uncertainty in $\Dest$.
Furthermore, using the textbook OLS expression for the uncertainty in $\Dest$ significantly underestimates the true uncertainty in this estimate.
This underestimated uncertainty may cause overconfidence in the accuracy of values of $\D$ estimated using OLS, and using these data in downstream analyses may result in faulty inferences.
While the uncertainty associated with OLS estimates of $\D$ can, in principle, be accurately estimated by directly sampling over multiple repeated simulations, this approach greatly increases the total computational cost and therefore is often not practical.

Here, we describe an approximate Bayesian regression method for estimating $\D$ with near-maximal statistical efficiency while also accurately estimating the corresponding statistical uncertainty, using data from a single simulation.
We model the statistical population of simulation MSDs as a multivariate normal distribution, using an analytical covariance matrix derived for an equivalent system of freely diffusing particles, with this covariance matrix parameterised from the observed simulation data.
We then use Markov-chain Monte Carlo to sample the posterior distribution of linear models compatible with this multivariate normal model.
The resulting posterior distribution provides an efficient estimate for $\D$ and allows the associated statistical uncertainty in $\Dest$ to be accurately quantified.
This method is implemented in the open-source Python package \textsc{kinisi}~\cite{mccluskey_kinisi_2022}.

\section{Background}

For a simulation of equivalent particles, the observed mean squared displacement as a function of time, $\oMSDs{t}$, can be computed as an average over equivalent particles and time origins:
%
\begin{equation}
  \oMSDs{t} = \frac{1}{N(t)}\sum^{N(t)}_{j=1}{\left[\Delta\mathbf{r}_j(t)\right]}^2,
  \label{equ:observed_msd}
\end{equation}
%
where $N(t)$ is the total number of observed squared-displacements at time $t$.
The resulting observed MSD is a vector, $\oMSD$, with individual elements $\oMSDi$.
Each element of this vector differs from the true ensemble-average MSD for that time by some unknown amount.
Fitting a linear model to $\oMSD$ gives an estimated self-diffusion coefficient, $\Dest$, which again differs from the true self-diffusion coefficient, $\D$, by some unknown amount.

Performing repeated simulations starting from different random seeds or with different histories will produce a set of replica trajectories, where each trajectory gives a different, statistically equivalent, observed MSD.
The set of all possible replica trajectories defines a population of hypothetical observed MSDs, and the MSD obtained from any one trajectory can be considered a random sample, $\bm{X}$, drawn from the multivariate probability distribution that describes this population, i.e, $\bm{X} \sim \prob{\oMSD}$.
Each potential MSD sample could, in principle, be fitted to a linear model to obtain a corresponding estimate for the self-diffusion coefficient; $\bm{X} \mapsto \Dest$.
The population of all such estimates therefore defines a probability distribution $\prob{\Dest}$.
The estimated diffusion coefficient obtained from a single simulation corresponds to a random sample drawn from this distribution, while the uncertainty in $\Dest$ is described by the shape of the full distribution $\prob{\Dest}$.

The statistical properties of $p(\Dest)$ depend on both the input MSD data and the choice of regression scheme used to obtain a ``best fit'' linear model.
An optimal estimation scheme for $\D$ should be unbiased, i.e., the expected value, $\mathbb{E}(\Dest)$, should equal the true self-diffusion coefficient $\D$, and should be maximally statistically efficient, i.e., the spread of $p(\Dest)$ around $\D$ should be minimised.
An estimation scheme should also provide an accurate estimate of the uncertainty in $\Dest$, to allow this estimated parameter to be used in subsequent inferential analysis.

For data that are both statistically independent and identically normally distributed, ordinary least squares regression (OLS) is unbiased and statistically efficient, and gives accurate estimates of the uncertainties in the resulting regression coefficients.
MSD data obtained from simulation, however, are neither statistically independent nor identically distributed.
The variances, $\var{\oMSDi}$, are correlated, since the displacement of each particle at time $t+\Delta t$ is necessarily similar to its displacement at time $t$, and hence, $\oMSDs{t}$ is similar to $\oMSDs{t+\Delta t}$.
These variances are also typically unequal---the data are heteroscedastic;
for example, for a random walk, the MSD variance increases quadratically with time \cite{smith_random_1996}. 
Because the key assumptions of the OLS method are not valid for MSD data, OLS gives statistically inefficient estimates of $\D$, while the estimated regression uncertainties obtained from the standard OLS statistical formulae significantly underestimate the true uncertainty in $\prob{\Dest_\mathrm{OLS}}$ (Fig.~\ref{fig:glswlsols}a).

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/glswlsols.pdf}
    \caption{
        Example distributions of estimated self-diffusion coefficients, $\Dest$, calculated using (a) ordinary least squares (OLS), (b) weighted least squares (WLS), and (c) generalised least squares (GLS),
        from MSD data from \num{4096} individual simulations of \num{128} particles undergoing a \SI{128}{step} 3D lattice random walk, with a step size chosen so that the true diffusion coefficient $\D = 1$.
        In each panel, the grey curve shows the best-fit normal distribution for the simulation data, the upper horizontal bar shows the standard deviation of this distribution, and the lower horizontal bar shows the average estimated standard distribution given by the analytical expression for $\sigma[\prob{\Dest}]$ for each regression method.}
    \label{fig:glswlsols}
    \script{glswlsols.py}
\end{figure}

Some improvement can be made by using weighted least squares (WLS) (Fig.~\ref{fig:glswlsols}b), where the residual for each observed MSD value is weighted by the reciprocal of its variance, $1/(\var{\oMSDi})$.
Like OLS, WLS is an unbiased estimator, and for heteroscedastic data it has higher statistical efficiency than OLS.
WLS still disregards correlations in $\oMSD$, however, and is therefore statistically inefficient, while the WLS estimated uncertainties for the regression coefficients still underestimate the true uncertainty in $\prob{\Dest_\mathrm{WLS}}$.

To optimally estimate the true ensemble-average MSD, and hence $\D$, from simulation data, it is necessary to account for both the changing variance and correlation structure of $\oMSD$.
Within the framework of linear regression, this can be achieved using generalised least squares (GLS).
GLS gives estimated regression coefficients, $\widehat{\beta}$, via
%
\begin{equation}
    \widehat{\beta} = \left(\modelmatrix^{\top}\mathbf{\Sigma}^{-1}\modelmatrix\right)^{-1}\modelmatrix^{\top}\mathbf{\Sigma}^{-1}\oMSD,
    \label{equ:gls}
\end{equation} 
%
where $\modelmatrix$ is the model matrix $\begin{bmatrix}\mathbf{1} & \bm{t}\end{bmatrix}$, with $\bm{t}$ the vector of observed times, and $\mathbf{\Sigma}$ is the covariance matrix for the observed MSD values.
For correlated heteroscedastic data, such as MSD data, GLS offers the theoretical maximum statistical efficiency---it achieves the Cram\'er--Rao bound~\cite{cramer_mathematical_1946,rao_information_1945,rao_selected_1994,darmois_sur_1945,aitken_on_1942}---and provides accurate analytical estimates of the uncertainty in the predicted regression coefficients (Fig.~\ref{fig:glswlsols}c).

A second method for estimating the ensemble-average MSD, and thus $\Dest$, from simulation data is Bayesian regression.
Like GLS, Bayesian regression can take into account both the changing variance and the correlation structure inherent in the data.
Rather than providing a singular ``best-fit'' estimate like GLS, Bayesian regression produces a posterior probability distribution for the regression coefficients.
The mean of this distribution serves as the point estimate of the coefficients and, in the absence of additional prior information, is equivalent to the GLS estimate, while the spread of the distribution quantifies the uncertainty in these estimates.
For data that is both heteroscedastic and correlated, such as MSD data from simulations, Bayesian regression, like GLS, is formally fully statistically efficient.

To estimate $\D$ from some observed MSD data, $\oMSD$, using Bayesian regression, we compute the posterior probability distribution $\prob{\model|\oMSD}$ for a linear model $\model = 6\D \bm{t} + c$, where $\D$ and $c$ are parameters to be estimated.
This posterior distribution is described by Bayes' theorem,
%
\begin{equation}
    \prob{\model|\oMSD} = \frac{\prob{{\oMSD|\model}}\prob{\model}}{\prob{\oMSD}},
    \label{equ:bayes}
\end{equation}
%
where $\prob{\oMSD|\model}$ is the probability of observing data $\oMSD$ given model $\model$, often described as the ``likelihood'', and $\prob{\oMSD}$ is the marginal probability of the observed data $\oMSD$.
Integrating over $\prob{\model|\oMSD}$ with respect to $c$ yields the marginal posterior distribution $\prob{\D|\oMSD}$, from which the best point-estimate $\Dest$ and distribution variance $\varest{\Dest}$ can be computed.

Given a sufficiently large number of observed squared displacements at each time $t$, the central limit theorem applies, and $\oMSD$ can be considered a sample from a multivariate normal distribution with log-likelihood
%
\begin{equation}
    \begin{aligned}{}
      \ln \prob{\oMSD|\model} = -\frac{1}{2}\big[ & \ln(\left|\mathbf{\Sigma}\right|) + {(\oMSD - \model)}^{\!\top}\mathbf{\Sigma}^{-1}(\oMSD - \model) \\ 
      & + k \ln(2\pi)\big],
    \end{aligned}
    \label{equ:loglike}
\end{equation}
%
where $\mathbf{\Sigma}$ is the observed MSD covariance matrix and $k$ is the length of the vector $\oMSD$, i.e., the number of time intervals for which we have observed MSD data.
Providing this likelihood function can be calculated, we can compute the posterior distribution $\prob{\model | \oMSD}$, which gives an optimally efficient point-estimate for $\D$ and a complete description of the associated uncertainty in $\Dest$.

\section{Approximating $\bm{\Sigma}$ from simulation data}

For Bayesian regression and GLS, we require the covariance matrix for the observed MSD, $\mathbf{\Sigma}$, which is generally unknown.
To proceed, we replace $\mathbf{\Sigma}$ with a model covariance matrix, $\mathbf{\Sigma^\prime}$, with a known analytical form, that we parameterise from the available simulation data.
Providing the correlation structure of $\mathbf{\Sigma^\prime}$ is similar to that of $\mathbf{\Sigma}$, this model correlation matrix can be used in approximate Bayesian or GLS schemes to estimate the ensemble-average MSD, and hence $\D$, with high efficiency and accurate estimated uncertainties.

We model the covariance matrix for the observed MSD from a given simulation using the covariance matrix for the MSD of an equivalent system of freely diffusing particles, $\mathbf{\Sigma^\prime}$.
For observed MSDs computed by averaging over non-overlapping time windows, the covariance matrix $\mathbf{\Sigma^\prime}$, in the long time limit, has elements (see SI)
%
\begin{equation}
  \Sigma^\prime\left[\oMSDi, \oMSDj\right]= \Sigma^\prime\left[\oMSDj, \oMSDi\right] =
  \var{\oMSDi} \frac{\nind{i}}{\nind{j}},\hspace{1em} \forall\,i \leq j,
  \label{equ:cvv}
\end{equation} 
%
where $\var{\oMSDi}$ are the time-dependent variances of the observed MSD, and $\nind{i}$ is the total number of non-overlapping observed squared-displacements for time-interval $i$.
We estimate the variances $\var{\oMSDi}$ using the standard result that the variance of the mean of a sample scales inversely with the number of independent constituent observations.
Specifically, we compute an estimated variance $\varest{\oMSDi}$ by rescaling the observed variance of the squared displacement for time interval $i$ by the number of non-overlapping contributing time windows, $\nind{i}$;
%
\begin{equation}
  \varest{\oMSDi} = \frac{1}{\nind{i}}\var{\Delta r_i^2}.
  \label{equ:varestMSD}
\end{equation}
%

The estimated variance $\varest{\oMSD}$ can be calculated from a single simulation trajectory, and provides an accurate estimate of the true variance $\var{\oMSD}$.
To demonstrate this, we performed \num{4096} independent simulations of \num{128} particles undergoing a three-dimensional cubic-lattice random walk of \num{128} steps per particle.
Using data from all \num{4096} simulations, we first compute the true simulation MSD and its variance (Fig.~\ref{fig:msd}a).
We also compute the MSD and estimated variance using data from a single simulation trajectory (Fig.~\ref{fig:msd}b), using the scheme described above.
A quantitative comparison between the true MSD variance and the single-trajectory estimated MSD variance is made in Fig.~\ref{fig:msd}c: the close numerical agreement confirms that Eqn~\ref{equ:varestMSD} can be used to estimate $\var{\oMSD}$, which can then be used to parameterise the model covariance matrix $\mathbf{\Sigma^\prime}$ via Eqn.~\ref{equ:cvv}.
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/msd.pdf}
    \caption{
        Comparison of the numerical variance in observed MSD from multiple replica simulations and the estimated variance in observed MSD given by rescaling the variance in observed squared displacements (Eqn.~\ref{equ:varestMSD}).
        Panel (a) shows the mean observed MSD from \num{4096} simulations of \num{128} particles undergoing a 3D lattice random walk of \num{128} steps per particle, with error bars of $\pm2\sigma[\oMSDi]$.
        Panel (b) shows the MSD from just one simulation, with error bars of $\pm2\widehat{\sigma}[\oMSDi]$, obtained via Eqn.~\ref{equ:varestMSD}.
        Panel (c) plots the numerical variance against the estimated variance from a single simulation as a function of timestep $i$.
        %Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences.
    }
    \label{fig:msd}
    \script{msd.py}
\end{figure}
%

The practical implementation of both GLS and Bayesian regression requires that the covariance matrix $\mathbf{\Sigma^\prime}$ is invertible (positive definite); see Eqns.~\ref{equ:gls} and \ref{equ:loglike}.
The estimated MSD variances derived from simulation data via Eqn.~\ref{equ:varestMSD} are statistically noisy and using these to directly parameterise $\mathbf{\Sigma^\prime}$ can yield non-invertible singular matrices.
To make our scheme numerically tractable, we therefore fit our estimated MSD variances to the analytical variance for an analogous system of particles undergoing random walks~\cite{smith_random_1996};
%
\begin{equation}
    \var{\oMSDi} = a\frac{t^2_i}{\nind{i}},
    \label{equ:variance}
\end{equation}
%
where $a$ is a scaling parameter determined by fitting Eqn.~\ref{equ:variance} to the directly estimated MSD variances.
This smoothing of $\var{\oMSDi}$ guarantees that the resulting model covariance matrix $\mathbf{\Sigma^\prime}$ is invertible and thus suitable for GLS or Bayesian regression.

To illustrate the complete numerical procedure for deriving the model covariance matrix, $\bm{\Sigma^\prime}$, we present in Fig.~\ref{fig:covariances} the MSD covariance matrix for \num{4096} random-walk simulations, as described above, at three differing levels of approximation:
the numerically converged covariance matrix, $\bm{\Sigma}$, computed using the data from all \num{4096} simulations (Fig.~\ref{fig:covariances}a);
the corresponding analytical model covariance matrix, $\mathbf{\Sigma^\prime}$, defined by Eqn.~\ref{equ:cvv} and parametrised using analytical variances $\var{\oMSDi}$ (Fig.~\ref{fig:covariances}b); and the average model covariance matrix obtained by parametrising Eqn.~\ref{equ:cvv} using smoothed variances estimated from individual simulation trajectories, and averaging over the resulting set of all \num{4096} matrices (Fig.~\ref{fig:covariances}c).
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/covariances.pdf}
    \caption{
        (a) The numerical MSD covariance matrix $\bm{\Sigma}$ calculated using MSD data from \num{4096} simulations of \num{128} particles undergoing a 3D lattice random walk of \num{128} steps per particle.
        (b) The analytical MSD covariance matrix $\bm{\Sigma^\prime}$ (Eqn.~\ref{equ:cvv}), parametrised using analytical random-walk variances $\var{\oMSDi}$.
        (c) The MSD covariance matrix obtained applying the numerical scheme described in the main text to each individual random walk simulation, averaged over all \num{4096} such simulations.
        Colour bars in (a--c) show the covariance, $\Sigma\left[\oMSDi, \oMSDj\right]$.
        The off-diagonal panels show difference plots, computed as per-element ratios between pairs of covariance matrices (a--c).
        %Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences.
    }
    \label{fig:covariances}
    \script{covariances.py}
\end{figure}
%

While the analytical and average estimated covariance matrices show some systematic deviation from the numerically converged covariance matrix, the general correlation structure is preserved.
The discrepancy between the model and numerical covariance matrices largely stems from the approximation made in deriving the analytical form that $t$ is large, which leads to an overestimation of the variance at low $t$.
Despite this, the average estimated covariance matrix reproduces well the correlation structure of the true numerical covariance matrix, indicating that the component covariance matrices estimated from individual simulation trajectories may be used within approximte GLS or Bayesian regression schemes to estimate $\D$ and $\var{\Dest}$.

\section{Validation}

To demonstrate the complete approximate Bayesian regression scheme, as described above, we present two distinct examples.
First, we consider a simple 3D-lattice random walk, where the true self-diffusion coefficient $\D$ is specified by the simulation parameters, and a well-converged numerical covariance matrix can be obtained with relatively low computational cost, which allows us to directly compare the estimates produced by our method to ``best case'' estimates from a hypothetical method with access to the true covariance matrix.
Second, we consider an example real-world system---the lithium-ion solid electrolyte \ce{Li7La3Zr2O12} (LLZO)---which represents a practical application of our method to a well-studied material of practical interest for solid-state lithium-ion batteries \cite{MuruganEtAl_AngewChemIntEd2007,burbano_sparse_2016,morgan_lattice_2017,SquiresEtAl_PhysRevMater2022}.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/random_walk.pdf}
    \caption{
        (a) Observed MSD from a single simulation of 128 particles undergoing a 3D-lattice random walk of 128 steps per particle (dark line).
        The green shading shows the corresponding posterior distribution $\prob{\model|\oMSD}$ of linear models compatible with the observed MSD data $\oMSD$, calculated using the scheme described in the main text.
        The variegated shading indicates compatibility intervals of \SIlist[list-final-separator = {, and }]{1;2;3}{\sigma}$[\prob{\model|\oMSD}]$.
        (b) The marginal posterior distribution $\prob{\Dest|\oMSD}$ obtained from the posterior distribution of linear models in (a).
        The mean of this distribution gives the point estimate $\Dest$ for this simulation input data.
        The blue horizontal bar shows an interval of one standard deviation in $\prob{\Dest|\oMSD}$.
        (c) Probability distribution of point-estimates $\prob{\Dest}$ obtained from \num{4096} individual random-walk simulations.
        Each simulation has been analysed as in (a) and (b) to yield a single corresponding point estimate $\Dest$.
        The grey line shows the distribution of point estimates, $\prob{\Dest_\mathrm{num}}$, obtained using Bayesian regression with a mean vector and numerical covariance matrix derived from the complete dataset of all \num{4096} simulations.
        The pink horizontal bar shows an interval of one standard deviation in $\prob{\Dest}$.
        (d) Probability distribution of estimated variances, $\varest{\Dest}$, for individual random-walk simulations, compared to the true sample variance (pink vertical line) $\var{\Dest}$.
    } 
    \label{fig:random_walk}
    \script{random_walk.py}
\end{figure*}

Fig.~\ref{fig:random_walk}a shows the observed MSD from a single 3D-lattice random-walk simulation, along with the estimated posterior distribution of linear models compatible with the observed MSD data, $\prob{\model|\oMSD}$, calculated via Eqns.~\ref{equ:bayes} and \ref{equ:loglike}.
The corresponding marginal posterior distribution of estimated diffusion coefficients $\prob{\D|\oMSD}$ is shown in Fig.~\ref{fig:random_walk}b; this distribution is approximately Gaussian and is centred on the true self-diffusion coefficient $\D = \num{1}$, demonstrating that for this example trajectory we obtain a good point-estimate of $\D$.

To evaluate the overall performance of our method, we repeat our analysis on the full set of \num{4096} random-walk simulations.
Fig.~\ref{fig:random_walk}c presents a histogram of the resulting point estimates of $\D$, with each estimate derived as the mean of the posterior distribution $\prob{\D|\oMSD}$ using input data from an individual simulation.
We also show the probability distribution of estimated diffusion coefficients obtained using Bayesian regression with a mean vector and covariance matrix derived numerically from all \num{4096} simulations (solid line).
This latter distribution represents the distribution of ``best possible'' estimates of $\D$ and exhibits the minimum possible theoretical variance.
The close agreement between these two distributions demonstrates that our approximate Bayesian regression scheme yields nearly optimal estimates of $\D$ using data from individual simulations.
The distribution of estimated diffusion coefficients from single simulations is slightly broader than the exact numerical results.
This minor deviation is a consequence of the overestimation of $\varest{\oMSDi}$ at short times, noted above, which results from our use of the long-time limit in the derivation of the analytical model covariance matrix.

We next consider the degree to which our method can quantify the uncertainty in $\Dest$ when using input data from a single simulation.
Fig.~\ref{fig:random_walk}d shows the distribution of estimated variances $\varest{\Dest}$, with each sample calculated from an individual simulation trajectory.
We also show the true variance of individual point estimates, $\var{\Dest}$, which characterises the spread of the histogram in Fig.~\ref{fig:random_walk}c.
The distribution of estimated variances is biased relative to the true variance and skewed, due to numerical differences between the true covariance matrix $\mathbf{\Sigma}$ and the model covariance matrix $\mathbf{\Sigma^\prime}$ (see the SI for details).
In general, however, the distribution of the estimated variance shows good agreement with the true sample variance.
Notably, the precision of this estimate is significantly greater than obtained using OLS or WLS and their corresponding textbook statistical formulae.

We next benchmark our method using data from simulations of the lithium-ion solid electrolyte cubic \ce{Li7La3Zr2O12} (c-LLZO).
We performed a single simulation of \num{1536} atoms (\num{448} Li ions) at \SI{1000}{K} for \SI{1.6}{\nano\second} (full simulation details are provided in the SI).
To generate multiple statistically equivalent trajectories, the resulting simulation data was partitioned into \num{512} effective trajectories, each approximately $\sim\SI{25}{\ps}$ in length, and containing data for \num{56} lithium ions.
We then perform the same approximate Bayesian regression analysis as above on each effective trajectory, excluding the first \SI{10}{ps} of MSD data in each case to remove short-time data corresponding to ballistic and sub-diffusive regimes \cite{he_statistical_2018}.

The resulting distribution of the point estimates, $\Dest$, from analysis of all \num{512} effective trajectories is shown in Fig.~\ref{fig:diffusion}a.
Again, the corresponding distribution of $\Dest$ estimates derived using Bayesian regression and a well-converged numerical covariance matrix calculated from the full LLZO dataset is also shown for comparison.
The distribution $\prob{\Dest}$ obtained using the model covariance matrix and parametrised separately for each individual effective simulation is highly similar to that obtained using the aggregate numerical covariance matrix calculated from the complete simulation dataset.
This close agreement mirrors the results for our random walk simulations, and confirms that our method yields accurate and statistically efficient estimates for $\D$, even for real-world simulation data.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/diffusion.pdf}
    \caption{
      (a) Probability distribution of point estimates $\prob{\Dest}$ for \num{512} effective simulations of LLZO (orange histogram).
      The grey line shows the distribution $\prob{\Dest_\mathrm{num}}$ obtained using Bayesian regression with the complete LLZO dataset as input.
      The pink bar shows an interval of one standard deviation $\sigma[\prob{\Dest}]$.
      (b) Probability distribution of estimated variances, $\varest{\Dest}$, for individual LLZO effective simulations, compared to the true sample variance (pink vertical line) $\var{\Dest}$.
      %Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences. 
    }
    \label{fig:diffusion}
    \script{diffusion.py}
\end{figure}

We also consider the probability distribution of estimates of the variance in $\Dest$ calculated for each effective trajectory (Fig.~\ref{fig:diffusion}b), which we compare to the true variance in $\Dest$ for our method; i.e., the variance of the histogram in Fig.~\ref{fig:diffusion}a.
While the estimated variances deviate somewhat from the true distribution $\prob{\var{\Dest}}$, the agreement is reasonable and mirrors our results for the random walk simulations.
Hence, our method provides reasonably accurate estimates of the uncertainty in $\Dest$ for our c-LLZO dataset, even when applied to single effective trajectories with limited displacement data (only 56 mobile ions, and \SI{25}{ps} simulation length).

% To further benchmark our method, we performed a scaling analysis using sets \num{512} random-walk simulations with varying total number of particles or total simulation length (number of steps).
% We then analyse the resulting simulation data using four estimation methods to generate distributions of compatible linear models:
% \begin{enumerate}
%   \item Bayesian regression using a numerical covariance matrix derived from the complete set of \num{512} simulations.
%   This analysis is equivalent to GLS, and gives the smallest theoretical variance in the resulting point estimates $\Dest$.
%   \item Approximate Bayesian regression as described above, with an estimated model covarianec matrix computed for each individual trajectory using the corresponding simulation data.
%   \item Approximate Bayesian regression with a diagonal covariance matrix, $\bm{\Sigma}=\bm{\sigma}\mathbf{I}$, with the diagonal terms estimated from the simulation data.
%   This analysis is equivalent to WLS.
%   \item Bayesian regression with a covariance matrix $\bm{\Sigma}=\mathbf{I}$, the identity matrix.
%   This analysis is equivalent to OLS.
% \end{enumerate}
% Our analysis is shown in Fig.~[TODO] across two panels. The first panel illustrates the scaling performance of each method as we vary the number of atoms. The second panel demonstrates the scaling performance of each method as we adjust the total simulation time. 

\section{Summary and Discussion}

We have introduced and evaluated an approximate Bayesian regression method for estimating the self-diffusion coefficient, $\D$, from molecular dynamics simulation data.
We consider the observed mean-squared displacement data from a single simulation as a random sample, $\bm{X}$, from a population of potential MSDs generated by equivalent replica simulations, $\bm{X}\sim p(\oMSD)$.
We model this population using a multivariate normal distribution, $p(\oMSD) = \mathcal{N}(\model, \mathbf{\Sigma})$, with mean vector $\model = 6\D\bm{t} + c$, where $\D$ and $c$ are model parameters to be determined.

To model the covariance matrix, we use an analytical solution derived for an equivalent system of freely diffusing particles.
To parameterise this model covariance matrix, we rescale the variance of the observed squared displacements from the input simulation trajectory, followed by a smoothing step to ensure a positive-definite matrix.
The resulting model covariance matrix preserves the correlation structure of the true simulation MSD covariance matrix, and gives a multivariate normal model for the population of observable simulation MSDs that depends solely on the model parameters, $\D$ and $c$.

We use Markov-Chain Monte Carlo to sample the posterior distribution of linear models compatible with the observed MSD data.
This approach yields a marginal posterior distribution, $\prob{\D | \oMSD}$, that gives a statistically efficient point estimate for $\D$ and allows the associated statistical uncertainty, $\var{\Dest}$, to be quantified.

We have benchmarked our approach using simulation data for an ideal 3D lattice random walk and for the lithium-ion solid electrolyte \ce{Li7La3Zr2O12} (LLZO).
In both cases, we obtain a distribution of estimates for $\D$ that closely matches the theoretically optimal distribution obtained using a numerical covariance matrix derived from a large number of replica simulation trajectories.

We obtain estimates for $\D$ that are unbiased, with near-optimal statistical efficiency, using input data from single simulation trajectories.
The approximate Bayesian regression scheme therefore provides more accurate single-point estimates of the self-diffusion coefficient than the commonly used OLS or WLS methods, when applied to the same input simulation data.
The improved statistical efficiency of this method, when compared to OLS or WLS, enables the estimation of $\D$ with equivalent accuracy from considerably smaller simulations---either in terms of timescale or system size. 
This reduces the overall computational cost when compared to studies that use OLS or WLS for estimating a linear fit to simulation MSD data.
Alternatively, this approach provides the possibility to estimate $\D$ with greater precision, given simulation trajectories of equal size.

Our method also provides reasonable estimates of the statistical uncertainty in the estimated value $\Dest$, in contrast to OLS and WLS which systematically significantly underestimate the uncertainty in regression coefficients when applied to simulated MSD data. 
While these estimated statistical uncertainties can still differ from the true (but unknown) uncertainty in $\Dest$, particularly when using short-timescale simulation data, they allow for scientifically meaningful comparisons to be made between estimated diffusion coefficients across different materials or under varying conditions, such as changes in temperature, or between computational findings and experimental results.
Furthermore, these uncertainties allow for quantitative downstream analysis, such as the application of Arrhenius (on non-Arrhenius) type models to describe the temperature dependence of self-diffusion.

The approximate Bayesian regression scheme presented here provides a statistically efficient means of estimating the self-diffusion coefficient, $\D$, from molecular dynamics simulation data.
It improves upon textbook approaches by providing accurate point estimates of $\D$ with near-optimal statistical efficiency, while also providing a reasonable description of the uncertainty in these estimates.
The high statistical efficiency of our method allows for the use of smaller simulations, which can significantly reduce computational costs.
Overall, our method offers significant advantages over more conventional methods of estimating self-diffusion coefficients from atomistic simulations.
We have implemented this procedure in the open-source package \textsc{kinisi} \cite{mccluskey_kinisi_2022}, which we hope will support its use within the broader simulation community across a range of materials science contexts.
\section*{Data availability}

Electronic Supplementary Information (ESI) available: A complete set of analysis/plotting scripts allowing for a fully reproducible and automated analysis workflow, using \textsc{showyourwork}~\cite{luger_showyourwork_2021}, for this work is available at Ref.~\cite{mccluskey_github_2022} under an MIT license.
All raw simulation files are available on Zenodo shared under CC BY-SA 4.0 licences, the random walk simulations and other analysis-linked data~\cite{mccluskey_zenodo_2022} and the LLZO raw simulation trajectories~\cite{coles_llzo_zenodo_2022}.
The method outlined in this work is implemented in the open-source Python package \textsc{kinisi}~\cite{mccluskey_kinisi_2022}, which is available under an MIT license.

\section*{CR\lowercase{e}d\lowercase{i}T author statement}

A.R.M.: Conceptualization, Formal Analysis, Investigation, Methodology, Software, Visualisation, Writing---original draft.
S.W.C.: Methodology, Resources, Writing---review and editing.
B.J.M.: Conceptualization, Methodology, Software, Writing---review and editing.

\section*{Acknowledgements}

The authors thank Jacob M.\ Dean and Tim Rogers for their valuable input in checking the mathematical derivations that make up the appendices, the beta-testers for the \textsc{kinisi} package, and Rodrigo Luger and Daniel Foreman-Mackey for their help using \textsc{showyourwork}~\cite{luger_showyourwork_2021}.
This work used the Isambard 2 UK National Tier-2 HPC Service (http://gw4.ac.uk/isambard/) operated by GW4 and the UK Met Office, and funded by EPSRC (EP/T022078/1).
The authors gratefully acknowledge the University of Bath's Research Computing Group~\cite{bath_research_2018} for their support in this work, in the LLZO molecular dynamics simulations.
Other simulations and analyses were carried out using the Data Management and Software Centre computing cluster at the European Spallation Source ERIC.
S.W.C. and B.J.M. acknowledge the support of the Faraday Institution through the CATMAT project (grant number FIRG016). 
B.J.M. acknowledges support from the Royal Society (UF130329 and URF\textbackslash R\textbackslash 191006). 

\bibliographystyle{naturemag}
\bibliography{bib}
\let\addcontentsline\oldaddcontentsline

\onecolumngrid
\clearpage 
\twocolumngrid

\appendix
\renewcommand\thesection{SI.\Roman{section}}
\counterwithout{figure}{section}
\renewcommand\thefigure{SI.\arabic{figure}}
\setcounter{figure}{0}
\counterwithout{equation}{section}
\renewcommand\theequation{SI.\arabic{equation}}
\setcounter{equation}{0}
\counterwithout{table}{section}
\renewcommand\thetable{SI.\arabic{table}}
\setcounter{table}{0}
\pagenumbering{arabic} 
\renewcommand\thepage{SI.\arabic{page}}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\title{Supplemental Material for ``\papertitle''}
\maketitle
\onecolumngrid

This document presents supplementary material for the manuscript ``\papertitle''.
It contains the following sections:
\begin{enumerate}
    % \item Evidence for the effect of the number of independent observations on the distribution of mean-squared displacements.
    \item The derivation of the covariance matrix in the long-time limit for freely diffusion particles.
    \item Quantification of the scaling behaviour in the posterior distribution for the diffusion coefficient with system size.
    \item Discussion of the skewed distribution of the estimated variance in the diffusion coefficient.
    \item Details of the LLZO molecular dynamics simulations.
    \item Discussion of the importance of only using non-diffusive simulation in the analysis.
    \item Details of the implementation of the model covariance method in the \textsc{kinisi} package.
\end{enumerate}
A repository containing the analysis and plotting code used to generate all results and figures in the main manuscript and this supplemental material document is available at \url{www.github.com/arm61/msd-errors}~\cite{mccluskey_github_2022}, under MIT (code) and CC BY-SA 4.0 (figures and text) licenses.
This repository includes a fully reproducible \code{showyourwork} workflow, which allows complete reproduction of the analysis, plotting of figures and compilation of the manuscripts.
The corresponding input datasets are openly available under the CC BY-SA 4.0 licence \cite{mccluskey_zenodo_2022,coles_llzo_zenodo_2022}. 

\vspace{\columnsep}
\twocolumngrid

% \section{Effect of number of independent observations on the distribution of mean-squared displacement}
% \label{sec:chi2}
% %
% In the main text, we note that for a large enough number of observations, the observed MSD is a multivariate normal distribution. 
% This is made clear on inspection of the observed MSD at a single time interval (which should follow a normal distribution), where squared particle displacements, in the diffusive regime, are $\chi^2_k$-distributed, where $k$ is the number of independent observations of a given displacement. 
% Fig.~\ref{fig:freedom} shows that when there are sufficiently many independent observations of ${\left[\Delta\mathbf{r}_i(t)_\alpha\right]}^2$, each population of observable MSD values $p(\bar{s}^2_i)$ is normally distributed, i.e.\ as the number of observations increases the $\chi^2_k$-distribution tends towards a normal distribution.
% A single observation of \SI{16}{atoms} (i.e.\ where $t>0.5t_{\max}$), the distribution is not yet normally distributed.
% %
% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{figures/freedom.pdf}
%     \caption{
%         A comparison between the observed mean-squared displacement (blue histograms) and the relevant $\chi^2_k$-distribution.
%         The simulations were run for \SI{128}{\second}, therefore there is only a single observation per atom per dimension, such that $k=N_{\mathrm{atoms}}d$; (a) $k = 2 \times 3$, (b) $k = 4 \times 3$, (c) $k = 8 \times 3$, (d) $k = 128 \times 3$.
%         % Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences. 
%     }
%     \label{fig:freedom}
%     \script{freedom.py}
% \end{figure*}
% %

\section{Derivation of the long-time limit covariance matrix for a system of freely diffusing particles.}
\label{sec:ran}
%
The main manuscript asserts that the long-time limit covariance matrix for a simulation of freely diffusing particles can be expressed as
%
\begin{equation}
  \Sigma^\prime\left[\oMSDi, \oMSDj\right]= \Sigma^\prime\left[\oMSDj, \oMSDi\right] =
  \var{\oMSDi} \frac{\nind{i}}{\nind{j}},\hspace{1em} \forall\,i \leq j,
  \label{equ:cvv_SI}
\end{equation}
%
where $\oMSDi$ is the observed mean-squared displacement (MSD) for time interval $i$ and $\nind{i}$ is the number of statistically independent observed squared displacements averaged over to compute the mean value (see Eqn.~2 in the main manuscript).

We must first consider the expected variance for the MSD at timestep $i$, $\var{\oMSDi}$, following the equivalent derivation by Smith and Gillan~\cite{smith_random_1996}, and then extend this to the covariance $\Sigma^\prime\left[\oMSDi, \oMSDj\right]$ to obtain the result given above.

For a single particle undergoing a one-dimensional random walk with step size $\kappa$, each step gives a displacement $h = \pm \kappa$.
After $n$ steps, the MSD, $\oMSDn$, is
%
\begin{equation}
  \begin{aligned}
    \oMSDn &= \left[\sum_i^n h_i\right]^2\\
           &= \sum_i^n\sum_j^n h_i h_j \\
           &= \sum_i^n h_i^2 + \sum_i^n\sum_{j\neq i}^n h_ih_j.
  \end{aligned}
\end{equation}
%
The expected MSD in the long-time limit, $\mathbb{E}(\oMSDn) = \moMSDn$, is obtained by averaging over all permutations of $h_i$ and $h_j$:
%
\begin{equation}
  \begin{aligned}
    \moMSDn &= \sum_i^n \left<h_i^2\right> + \sum_i^n\sum_{j\neq i}^n\left<h_i h_j\right>
  \end{aligned}
\end{equation}
%
For a random walk, the second term averages to zero for all $h_i$ and $h_j$, and
%
\begin{equation}
  \begin{aligned}
    \moMSDn &= \sum_i^n \left<h_i^2\right> \\
            &= n\kappa^2.
  \end{aligned}
  \label{equ:msd_SI}
\end{equation}
%
Hence the expected value for the mean-squared displacement increases linearly with the number of steps taken.
% First, we will outline a brief note on nomenclature, we will use $\MSD{n}$ to refer to the mean-squared displacement in a single simulation, i.e., this is the mean over all particles and observations of the given number of hops, $n$, $\Big\langle\MSD{n}\Big\rangle$ will be used for the average of the mean-squared displacements from many simulations. 
% We can think of these as a sample and population mean respectively, where $\MSD{n} \to \Big\langle\MSD{n}\Big\rangle$ with increasing numbers of simulations.  

The variance in the observed MSD, $\var{\oMSDn}$ is given by the standard statistical formula
%
\begin{equation}
    \var{\oMSDn} = \left<{\left[\oMSDn - \moMSDn\right]}^2\right>,
\end{equation}
%
which can be expanded as
%
\begin{equation}
    \begin{aligned}
        \var{\oMSDn} &= \left<\oMSDn^2\right> - 2\moMSDn\left<\oMSDn\right> + \moMSDn^2,\\
                     &= \left<\oMSDn^2\right> - \moMSDn^{\,2}.
    \end{aligned}
\end{equation}
%

The first term can be expanded in terms of displacements $h$ as
%
\begin{equation}
  \left<\oMSDn^2\right> = \left<\sum_i^n\sum_j^n\sum_k^n\sum_l^n h_i h_j h_k h_l\right>
  \label{equ:big_av}
\end{equation}
%
which can be simplified by noting that $h_i$, $h_j$, $h_k$, and $h_l$ are uncorrelated when $i \neq j \neq k \neq l$, and the only terms that contribute to the average are those with:
%
\begin{itemize}
    \item[(a)] $i = j = k = l$;
    \item[(b)] $(i = j) \neq (k = l)$;
    \item[(c)] $(i = k) \neq (j = l)$;
    \item[(d)] $(i = l) \neq (j = k)$. 
\end{itemize}
%
Each of these conditions guarantees a positive product $h_ih_jh_kh_l$.

From (a) we obtain
%
\begin{equation}
    \left<\sum_i^nh_i^4\right> = n\kappa^4,
\end{equation}
%
and from (b), (c), and (d), which are equivalent, we obtain
%
\begin{equation}
    \left<\sum_i^n\sum_j^nh_i^2h_j^2\right> = (n\kappa^2)^2 = n^2\kappa^4.
\end{equation}
%
This gives
%
\begin{equation}
    \left<\oMSDn^2\right> = (3n^2 + n)\kappa^4,
\end{equation}
%
which, in the limit $n \to \infty$, approaches
%
\begin{equation}
    \left<\oMSDn^2\right> = 3n^2\kappa^4.
    \label{equ:infty}
\end{equation}
%
Combining this with the square of Eqn.~\ref{equ:msd_SI} gives the variance in the mean-squared displacement as
%
\begin{equation}
    \var{\oMSDn} = 3n^2\kappa^4 - n^2\kappa^4 = 2n^2\kappa^4,
\end{equation}
%
i.e., the variance increases quadratically with the number of steps taken.

The result above gives the variance in the mean squared displacement of a single particle if we consider a single time origin.
In practical simulations with more than one mobile particle we can obtain improved statistics by averaging over independent observed squared displacements; either averaging over equivalent mobile particles or averaging over statistically independent sections of the observed simulation trajectory, which we consider to be the case for mutually non-overlapping time intervals.
Now the variance in MSD is scaled by the usual number of independent contributing observations to give
%
\begin{equation}
    \var{\oMSDn} = \frac{2n^2\kappa^4}{\nind{n}},
\label{equ:der_var}
\end{equation}
%
where $\nind{n}$ is the total number of statistically independent squared displacements that contribute to $\oMSDi$.

The results in Eqns.~\ref{equ:msd_SI} \&~\ref{equ:der_var} can be extended to a $d$-dimensional lattice random walk, where for each step is along one of the $d$-dimensions, by combining the results for one dimension. 
This means that the MSD becomes 
%
\begin{equation}
    \moMSDn_{d} = \sum^d{\frac{n\kappa^2}{d}} = n\kappa^2,
\end{equation}
%
and the variance in the MSD becomes
%
\begin{equation}
    \var{\oMSDn}_d = \sum^d{\frac{2n^2\kappa^4}{d^2\nind{n}}} = \frac{2n^2\kappa^4}{d\nind{n}},
\end{equation}
%
where the $n$ in both Eqn.~\ref{equ:msd_SI} \&~\ref{equ:der_var} have been replaced with $n/d$ as the steps are spread (equally) over all dimensions. 

This can be extended to consider the covariance between two different numbers of steps, $n$ and $n+m$, in the random walk where the expected MSDs will be
%
\begin{equation}
    \begin{aligned}
        \moMSDn &= n\kappa^2; \\
        \moMSDm &= (n+m)\kappa^2.
    \end{aligned}
\end{equation}
%
The covariance between these is defined as
%
\begin{equation}
    \Sigma \left[\oMSDn, \oMSDm \right] = \left<{\left[{\oMSDn - \moMSDn}\right] \left[{\oMSDm - \moMSDm}\right]}\right>, 
\end{equation}
%
which can be expanded,
%
\begin{equation}
    \begin{aligned}
        \Sigma \left[\oMSDn, \oMSDm \right] = \big\langle & \oMSDn\oMSDm - \oMSDn\moMSDm \\
        & - \moMSDn\oMSDm + \moMSDn\moMSDm\big\rangle,
    \end{aligned}
\end{equation}
%
and then reformulated to give
%
\begin{equation}
    \Sigma \left[\oMSDn, \oMSDm \right] = \left<\oMSDn\oMSDm\right> - \moMSDn\moMSDm,
    \label{equ:pair}
\end{equation}
%
where 
%
\begin{equation}
    \begin{aligned}
        \moMSDn\moMSDm &= \oMSDn\oMSDm \\
                       &= nd^2(n+m)\kappa^2 \\
                       &= n(n+m)\kappa^4
    \end{aligned}
\end{equation}
%
and, by analogy to Eqn.~\ref{equ:big_av},
%
\begin{equation}
    \left<\oMSDn\oMSDm\right> = \left<\sum_i^n\sum_j^n\sum_k^{n+m}\sum_l^{n+m} h_i h_j h_k h_l\right>,
\end{equation}
%
which we can rewrite as
%
\begin{equation}
    \begin{aligned}
        \left<\oMSDn\oMSDm\right> = \Bigg\langle & \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^{n}\sum_{l=1}^{n} h_i h_j h_k h_l \\
        & + \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^{n}\sum_{l=n+1}^{n+m} h_i h_j h_k h_l \\
        & + \sum_{i=1}^n\sum_{j=1}^n\sum_{k=n+1}^{n+m}\sum_{l=1}^{n} h_i h_j h_k h_l \\
        & + \sum_{i=1}^n\sum_{j=1}^n\sum_{k=n+1}^{n+m}\sum_{l=n+1}^{n+m} h_i h_j h_k h_l \Bigg\rangle.
    \end{aligned}
    \label{equ:vbig}
\end{equation}
%
The second and third terms in Eqn.~\ref{equ:vbig} tend to \num{0} as there is an equal probability of positive and negative displacements. 
This reduces Eqn.~\ref{equ:vbig} to 
%
\begin{equation}
    \begin{aligned}
        \left<\oMSDn\oMSDm\right> = & \Bigg\langle \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^{n}\sum_{l=1}^{n} h_i h_j h_k h_l \Bigg\rangle \\
        & + \Bigg\langle \sum_{i=1}^n\sum_{j=1}^n\sum_{k=n+1}^{n+m}\sum_{l=n+1}^{n+m} h_i h_j h_k h_l \Bigg\rangle, 
    \end{aligned}
\end{equation}
%
and using Eqn.~\ref{equ:infty} gives
%
\begin{equation}
    \left<\oMSDn\oMSDm\right> = 3n^2\kappa^4 + \Bigg\langle \sum_{i=1}^n\sum_{j=1}^n\sum_{k=n+1}^{n+m}\sum_{l=n+1}^{n+m} h_i h_j h_k h_l \Bigg\rangle.
\end{equation}
%
We can rewrite this as
%
\begin{equation}
    \left<\oMSDn\oMSDm\right> = 3n^2\kappa^4 + \Bigg\langle \sum_{i=1}^n\sum_{j=1}^n h_i h_j \Bigg\rangle \Bigg\langle \sum_{k=n+1}^{n+m}\sum_{l=n+1}^{n+m} h_k h_l \Bigg\rangle,
\end{equation}
%
where the following holds,
%
\begin{equation}
    \begin{aligned}
        \left<\oMSDn\oMSDm\right> &= 3n^2\kappa^4 + n\kappa^2 m\kappa^2 \\
                                  &= 3n\kappa^4 + nm\kappa^4.
    \end{aligned}
\end{equation}
%
Putting this result into Eqn.~\ref{equ:pair} allows the covariance to be written as
%
\begin{equation}
    \begin{aligned}
    \Sigma^\prime \left[\oMSDn, \oMSDm \right] &= 3n^2\kappa^4 + nm\kappa^4 - n(n+m)\kappa^4 \\
                                        &= 3n^2\kappa^4 - n^2\kappa^4 = 2n^2\kappa^4,
    \end{aligned}
    \label{equ:cov_der}
\end{equation}
where we use the $\Sigma^\prime$ notation to identify that this is in the long-time limit. 

In this case, the covariance depends only on the number of overlapping points, $n$, between the two time intervals. 
We can easily rationalise this by accepting that for a random walk any non-overlapping point will be completely uncorrelated and therefore have a covariance of \num{0}. 
Similar to the case for the variance, the covariance derived in Eqn.~\ref{equ:cov_der} is that for a single particle at a single time origin. 
The number of independent observed squared displacements for a given covariance should be the minimum number of shared independent observed squared displacements between the two time intervals, which is $\nind{n+m}$. 
Therefore, scaled by the number of contributing independent observations, the covariance, in the long-time limit, is
%
\begin{equation}
    \Sigma^\prime \left[\oMSDn, \oMSDm \right] = \frac{2n^2\kappa^4}{\nind{n+m}}.
\end{equation}
%
Similar to the MSD and the variance, the covariance can be written for $d$-dimensions as 
%
\begin{equation}
    \Sigma^\prime \left[\oMSDn, \oMSDm \right] = \frac{2n^2\kappa^4}{d\nind{n+m}}.
\end{equation}
%
The covariance can be calculated directly from the variance by recognising that both depend on the number of overlapping points, $n$, as follows
%
\begin{equation}
    \Sigma^\prime \left[\oMSDn, \oMSDm \right] = \var{\oMSDn}\frac{\nind{n}}{\nind{n+m}}.
\end{equation}
%
This is then rewritten in terms of $i$ and $j$ to give, Eqn.~\ref{equ:cvv_SI}.

Using the equivalence of $2d\D t \equiv n\kappa^2$~\cite{howard_reports_1964}, Eqns.~\ref{equ:msd_SI} \&~\ref{equ:cvv_SI} can be rewritten in terms of $t$ (or $t_1$ and $t_2$) and the diffusion coefficient, for any dimensionality of lattice random walk,
%
\begin{equation}
    \oMSDs{t} = 2d \D t,
    \label{equ:der_msd_fick}
\end{equation}
%
and 
%
\begin{equation}
    \Sigma^\prime \left[\oMSDs{t_1}, \oMSDs{t_2} \right] = 8d{(\D)}^2 t_1^2\frac{N^\prime(t_2)}{N^\prime(t_1)},\hspace{1em} \forall\,t_1 \leq t_2.
\end{equation}
%

\section{Scaling of the estimation of $\prob{\Dest}$}
\label{sec:scaling}
%
The true sample variance from \num{512} diffusion coefficient estimates, $\var{\Dest}$, using either the model covariance method, outlined in the main text, the independent method (using estimated variances but ignoring the need for the full covariance matrix, analogous to WLS), the independent \& identical method (analogous to OLS), and using the full numerical covariance matrix as a function of simulation length and number of atoms are compared in Fig.~\ref{fig:stat_eff}.

All estimation methods have the same power law exponent, $a\approx-1$, when the number of atoms is varied, due to the number of observations linearly scaling with number of atoms. 
This is the observation of the denominator ($n - 1$, where $n$ is the sample size) in the standard variance formula. 
The numerical and model covariance methods show the same relationship when the maximum simulation length is varied.
However, the independent and independent \& identical approaches have values for $a$ of \numrange{0.1}{0.3}.
Doubling the simulation length doubles the number of observations in the first half of the time range, while, the second half contains only new samples.
The full off-diagonal values of the covariance matrix are necessary to correctly account for these new samples (as the off-diagonal elements link between the different time intervals), leading to the poor scaling of the WLS and OLS analogues that is observed. 
%
% \begin{table*}
%     \centering
%     \caption{Parameters for the scaling of the variance of the diffusion coefficient for truly random walks, from Figs.~\ref{fig:stat_eff}a \& b.}
%     \label{tab:ipl_true}
%     \begin{tabular}{c  c | rclcl | c | c | c}
%          & & numerical & This & Independent & Independent \& Identical \\
%         \hline
%         \multirow{2}{*}{$1/N_{\mathrm{atoms}}$} & $a$ & \num{-9.81 \pm 0.06e-1} & \num{-9.03 \pm 0.18e-1} & \num{-9.95 \pm 0.12e-1} & \num{-9.78 \pm 0.14e-1} \\
%           & $b$/atoms$^{-a}$ & \num{1.42 \pm 0.03e-3} & \num{1.36 \pm 0.08e-3} & \num{7.27 \pm 0.26e-3} & \num{2.15 \pm 0.09e-2} \\
%         \multirow{2}{*}{$1/\Delta t_{\max}$} & $a$ & \num{-1.20 \pm 0.01e-0} & \num{-1.05 \pm 0.02e-0} & \num{-5.69 \pm 0.17e-1} & \num{-8.33 \pm 1.80e-2} \\ 
%         & $b$ & \num{3.78 \pm 0.15e-3} & \num{2.74 \pm 0.14e-3} & \num{8.99 \pm 0.51e-4} & \num{2.81 \pm 0.24e-4} \\ 
%     \end{tabular}
% \end{table*}
%
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/stat_eff.pdf}
    \caption{
        Scaling of statistical efficiency of different approaches;
        the decrease in $\sigma^2[\hat{\mu}(\Dest_\alpha)]$ as a function of number of atoms (a \& c) and maximum simulation length (b \& d).
        The solid lines indicate the fitted power law relationship for each of the numerical results (blue), the model covariance approach (green), where the variances are found by rescaling but the data considered independent (orange), and where the variances are considered to be identical and independent (pink). 
        For all except the numerical result, the variance was found from \num{512} determinations of the mean diffusion coefficient from equivalent simulations of walks of \SI{128}{atoms} for \num{128} steps and a step size of \num{1}.
        % Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences. 
        }
    \label{fig:stat_eff}
    \script{stat_eff.py}
\end{figure}
% 

The scaling in the variance that is estimated from each individual simulation using the model covariance method also shows different behaviour between changing the number of atoms and changing the maximum simulation length (Fig.~\ref{fig:efficiency}).
Initially, we compare the scaling behaviour of the estimated variance with the numerically determined variance (Figs.~\ref{fig:efficiency}a \&~\ref{fig:efficiency}b), where the power law is similar regardless of how the system changes in size. 
The width of the distribution of $\prob{\varest{\Dest}}$ (the error bars in Figs.~\ref{fig:efficiency}a \&~\ref{fig:efficiency}b) also show power law behaviour (Figs~\ref{fig:efficiency}c \&~\ref{fig:efficiency}d). 
In this case, the exponent is different if the number of atoms is changed ($a \approx -2$) compared to the maximum simulation length ($a \approx -1$). 
The precision of the variance estimate will improve more if the number of atoms is doubled rather than if the simulation is run for twice as long. 
When the number of atoms is doubled, the number of contributing independent observations also doubles for all time intervals but when the simulation is doubled in length, the number of contributing independent observations is only doubled up to the original simulation length.
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/efficiency.pdf}
    \caption{
        Scaling of estimated variance (a \& b), estimated variance precision (c \& d), and estimated variance accuracy (e \& f) from the method outlined in the main text;
        Green points indicate mean values for the estimated variance, where error bars are present they indicate a \SI{95}{\percent} confidence interval. 
        Blue points indicate the variance found from numerical simulations.
        The precision is quantified with the difference between the upper and lower bounds of the confidence interval and the accuracy from the bias compared to the numerical result.
        The variance was found from \num{512} determinations of the diffusion coefficient from equivalent simulations of walks. 
        When $N_{\mathrm{atoms}}$ was varied $t_{\max}$ was \SI{128} steps and when $t_{\max}$ was varied $N_{\mathrm{atoms}}$ \SI{128}{atoms}; for all simulations the step size was \num{1}.
        % Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences. 
        }
    \label{fig:efficiency}
    \script{efficiency.py}
\end{figure}
% 

\section{Skewness of distribution of $\varest{\Dest}$}
\label{sec:skew}
%
The skew observed in the estimated variance for the diffusion coefficient when using the model covariance matrix (Fig.~\ref{fig:random_walk}d) arises from differences between the numerical and model covariance matrices
If the numerical covariance matrix was used in place of the model covariance matrix, the resulting distribution of $\varest{\Dest}$ is not be skewed (Fig.~\ref{fig:true_cov}).
Additionally, the distribution of $\Dest$ is no longer broader than the numerical distribution, as is the case in Fig~\ref{fig:random_walk}c, which arises from the model covariance matrix (and analytical covariance matrix) overestimating the variance at low $t$ (Fig.~\ref{fig:covariances}), due to the long-time limit assumption made in its derivation.
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/true_cov.pdf}
    \caption{
        The cause of the skewed variance estimation when the model covariance matrix is used; 
        distributions of $\mu(\Dest)$ (a) and $\sigma^2(\Dest)$ (b) determined using the observed mean-squared displacements and the numerical covariance matrix (from \num{1024} equivalent simulations), showing the unskewed distribution of variances, the numerical distribution of $\mu(\Dest)$ is shown with the blue solid line and the sample variance in (a) is shown with the vertical green line in (b).
        Values found using random walks of \SI{128}{atoms} for \num{128} steps and a step size of \num{1}. 
        % Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences. 
        }
    \label{fig:true_cov}
    \script{true_cov.py}
\end{figure}
%

\section{LLZO simulations}
%
Classical molecular dynamics were run using the \textsc{metalwalls} code~\cite{marin_metalwalls_2020}. 
The DIPPIM polarisable ion force field parameterised by Burbano \emph{et al.}~\cite{burbano_sparse_2016} was used due to its ability to account accurately for the effect of ion polarisability on diffusion~\cite{wilson_polarization_1993,burbano_sparse_2016}. 
Simulations were run for the cubic phase of LLZO in NVT ensemble at \SI{1000}{\kelvin} and were extended to a length of \SI{2}{\nano\second} with a \SI{2}{\femto\second} timestep. 
Temperature was maintained by means of Nos\'{e}-Hoover thermostat with a relaxation time of \SI{121}{\femto\second} (5000 $\hbar / E_{h}$)~\cite{nose_unified_1984,hoover_canonical_1985,martyna_nose_1992}. 
As in the prior study of Burbano \emph{et al.}~\cite{burbano_sparse_2016}, simulations were performed using $2 \times 2 \times 2$ supercells with \SI{1536}{atoms} in a manner similar to Ref.~\citenum{burbano_sparse_2016} with the system being in the perfect crystalline form at the beginning.

\section{Effect of non-diffusive simulation}
\label{sec:nondiff}
%
In the modelling of the diffusive process, it is necessary to exclude simulation not in the diffusive regime, where the atoms are ``caged'' by their neighbours. 
The model covariance matrix has been derived assuming the diffusive regime (the Einstein relation implicitly assumes diffusion), therefore only diffusion should be included in our analysis. 
The non-diffusive regime typically dominates on short-time scales, giving way to diffusion at larger time scales. 
Fig.~\ref{fig:true_llzo} investigates the effect of using starting the Bayesian linear model at increasing initial values of $t$ (i.e.\ the value of $t$ from which the linear model was applied), in essence reducing the percentage of simulation from non-diffusive particles in the analysis. 
Using simulation from non-diffusive particles leads to a bias in the estimation of the variance of $\prob{\Dest}$, which is reduced when less of the non-diffusive regime is used. 
%
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/true_llzo.pdf}
    \caption{
        A comparison of the effect of different starting times for the diffusive regime \SIlist[list-final-separator = {, and }]{0;2;4;6;8;10;15;20}{\pico\second} of the LLZO systems on $\varest{\Dest}$.
        % Raw simulation data~\cite{mccluskey_zenodo_2022} and scripts~\cite{mccluskey_github_2022} to generate this figure are available under CC BY 4.0/MIT licences. 
        }
    \label{fig:true_llzo}
    \script{true_llzo.py}
\end{figure*}
%

To avoid the spurious effect of non-diffusive data, this regime must be excluded. 
It is useful, in the first instance, to overestimate excluded regime and assess if the resulting estimated variance is small enough to draw conclusions about the data or perform hypothesis testing.  
A balance can be struck between the start of the diffusive regime (having this as high as possible to ensure minimal non-diffusive data) and the size of the resulting variance. 

\section{Numerical implementation in \textsc{kinisi}}
\label{sec:implementation}
%
We have implemented the method outlined in the main text in the open-source Python package \textsc{kinisi}~\cite{mccluskey_kinisi_2022}. 
Here, we will discuss both \textsc{kinisi}'s broader implementation of the method and its application in the validation examples. 

First, to find the maximum number of observations of squared displacements at each time interval, \textsc{kinisi} uses overlapping sliding window sampling. 
The maximum number of observations for a given time interval, $t$, is $N_{\mathrm{atoms}} \times (N_{t} - i)$ displacements, where $N_{\mathrm{atoms}}$ is the number of atoms, $N_{t}$ is the total number of timesteps, and $i$ is the index of the timestep (starting with \num{1} for the shortest timestep). 
The independent variance is estimated by rescaling the observed overlapping variance in the squared displacements by the number of non-overlapping squared displacements, $\nind{i} = N_{\mathrm{atoms}} \times N_{t} / i$, as shown in Eqn.~\ref{equ:varestMSD}. 
The population mean for the squared displacements and the independent variances are then stored. 

Eqn.~\ref{equ:cvv_SI} defines the parameterisation of the covariance matrix from the variances and the number of independent observations. 
The covariance matrix is only constructed for values of $t$ where the particles are considered to be diffusing, which can be controlled manually by the user. 
In this work, the particles were assumed to be diffusing for $t>\SI{4}{\second}$ for the random walks and \SI{10}{\pico\second} for the LLZO system in Fig.~\ref{fig:diffusion}. 
The LLZO system was investigated at a range of starting times in Fig.~\ref{fig:true_llzo}. 

To obtain an initial guess for the gradient and intercept of the linear model that describes the observed MSD, \textsc{kinisi} uses ordinary least squares. 
This initial guess is then used as a starting point for the minimisation of the negative maximum a posteriori (the maximum of the posterior distribution in Eqn.~\ref{equ:bayes}), with the prior probability that the diffusion coefficient is greater than or equal to zero~\cite{broyden_convergence_1970,fletcher_new_1970,goldfarb_family_1970,shanno_conditioning_1970}.
The log-likelihood calculation (Eqn.~\ref{equ:loglike}) makes use of the Moore-Penrose generalisation of the inverse of a Hermitian matrix~\cite{moore_on_1920,bjerhammar_application_1951,penrose_generlized_1955}. 

To sample the joint posterior probability distribution of the linear model, \textsc{kinisi} uses the \textsc{emcee} package~\cite{foremanmackey_emcee_2019} was used, which implements Goodman and Weare's affine invariant Markov chain Monte Carlo ensemble sampler~\cite{goodman_ensemble_2010}. 
A physically-informed prior probability that $\D > 0$ is imposed on all analysis.
The sampling is performed with \num{32} walkers, for \num{1500} steps, the first \num{500} steps are removed as a burn-in period, finally, the sampled chains are thinned, such that only every \num{10}th value is used. 
This gives \num{3200} points sampled in the posterior distribution, these points can be histogrammed (as in Fig~\ref{fig:random_walk}) and summary statistics taken.

\end{document}